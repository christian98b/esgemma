{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning notebook\n",
    "\n",
    "The basics of this notebook is based on the notebooks provided by unsloth\n",
    "https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length : int = 8192 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit : bool = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-2-2b-it-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages : list[dict[str,str]] = [\n",
    "        {\"role\": \"user\", \"content\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\",},\n",
    "        {\"role\" : \"assistant\", \"content\" : \"Hi this is me\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=False,\n",
    "        return_tensors=\"pt\",\n",
    "        tokenize=False\n",
    "    )\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from unsloth import FastLanguageModel, is_bf16_supported, is_bfloat16_supported\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "import torch\n",
    "import datetime\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.1, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "dataset = load_dataset('chris7374/esg-net-zero', revision='100_per_class_v3')\n",
    "val_dataset = load_dataset('chris7374/esg-net-zero', revision='validation')\n",
    "\n",
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "You are an expert ESG (Environmental, Social, and Governance) analyst who conducts ESG research by analyzing texts to identify the presence of climate balance targets. Your primary task is to classify identified targets into one of four predefined classes and determine the target year for the climate balance target. Only consider overall climate balance targets, meaning that they are company-wide.\n",
    "The possible classes are “Carbon neutral(ity)”, “Emissions reduction target”, “Net zero”, and “No target”.\n",
    "Each class has equal importance, and the correct classification should reflect the most explicit target mentioned in the text. In cases where multiple classes are present:\n",
    "\t•\t“Net zero” should only be prioritized if explicitly mentioned as a company’s overarching target.\n",
    "\t•\t“Carbon neutral(ity)” takes precedence over “Emissions reduction target” only if it is the primary focus of the text.\n",
    "\t•\t“Emissions reduction target” should be classified if it is directly stated and not overshadowed by “Net zero” or “Carbon neutral(ity)” commitments.\n",
    "\t•\tIf no explicit target is mentioned, classify as “No target”.\n",
    "Ensure the classification is based on explicit information from the text, without assuming that one target implies another unless clearly stated.\n",
    "\n",
    "### Context:\n",
    "{}\n",
    "\n",
    "### Response Formatting: \n",
    "Only answer in the following XML format:\\n<answer><classification><end_target>Target</end_target></classification><extraction><end_target_year>Year</end_target_year></extraction><quote>...</quote></answer>\n",
    "\"\"\"\n",
    "\n",
    "output = \"\"\"\n",
    "<answer>\n",
    "<classification>\n",
    "<end_target>{}</end_target>\n",
    "</classification>\n",
    "<extraction>\n",
    "<end_target_year>{}</end_target_year>\n",
    "</extraction>\n",
    "<quote>{}</quote>\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    end_target = examples['end_target']\n",
    "    end_target_year = examples['end_target_year']\n",
    "    context = examples['custom_text']\n",
    "    quote = examples['custom_short_description']\n",
    "    texts = []\n",
    "    for end_target, end_target_year, context,quote in zip(end_target, end_target_year, context,quote):\n",
    "        messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"{prompt.format(context)}\",},\n",
    "        {\"role\" : \"assistant\", \"content\" : output.format(end_target,end_target_year,quote)}\n",
    "        ]\n",
    "        #https://huggingface.co/docs/transformers/main/chat_templating\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=False,\n",
    "            return_tensors=\"pt\",\n",
    "            tokenize=False,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        texts.append(text)\n",
    "    return {\"text\" : texts}\n",
    "pass\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
    "dataset = dataset['train']\n",
    "dataset = dataset.shuffle(seed=1234)  # Shuffle dataset with same seed for reproducable runs.\n",
    "train_data = dataset\n",
    "val_dataset = val_dataset.map(formatting_prompts_func, batched = True)\n",
    "val_dataset = val_dataset['train']\n",
    "val_dataset = val_dataset.shuffle(seed=1234)  # Shuffle dataset with same seed for reproducable runs.\n",
    "\n",
    "# Naming for the output folder\n",
    "project = \"esg\"\n",
    "base_model_name = \"gemma2\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"outputs\" + run_name\n",
    "\n",
    "num_train_epochs = 3\n",
    "batch_size = 2\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "#This is needed for training on completions only and needs to be updated for different models than gemma-2b-it\n",
    "#Response template should start with the first tokens. So if there is also a system prompt the response template should start with that\n",
    "#The response template can be read from the tokenizer and its get_chat_template function. Or if the apply_chat_template is applied without tokenizing.\n",
    "instruction_template = \"<start_of_turn>user\"\n",
    "response_template = \"<start_of_turn>model\"\n",
    "#This datacollator trains on the completions only and ignores the input tokens https://huggingface.co/docs/transformers/main_classes/data_collator\n",
    "#Essentially the input tokens get masked and ignored by the model https://huggingface.co/docs/trl/v0.11.4/en/sft_trainer#train-on-completions-only\n",
    "collator = DataCollatorForCompletionOnlyLM(instruction_template = instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False) \n",
    "#https://huggingface.co/docs/trl/main/en/sft_trainer#trl.SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    data_collator=collator,\n",
    "    #https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = batch_size,\n",
    "        per_device_eval_batch_size = 2,\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "        learning_rate = 2e-4,\n",
    "        warmup_ratio= 0.1,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        do_eval=True,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps = 5,\n",
    "        save_strategy=\"epoch\", #This means that the model gets saved every epoch. Alter this to steps for large datasets\n",
    "        #report_to=\"wandb\",  \n",
    "        logging_steps=5,\n",
    "        seed = 3407,\n",
    "        output_dir = output_dir,\n",
    "        run_name=f\"{run_name}-{datetime.datetime.now().strftime('%Y-%m-%d-%H-%M')}\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples:\n",
    "Input: \"Our company aims to achieve carbon neutrality by 2040 through a combination of emissions reduction and carbon offsets.\"\n",
    "Output:\n",
    "<end_target>Carbon neutral(ity)</end_target>\n",
    "<end_target_year>2040</end_target_year>\n",
    "<quote>company aims to achieve carbon neutrality by 2040</quote>\n",
    "Input: \"We're committed to reducing our greenhouse gas emissions by 50% compared to 2015 levels by the year 2030. Our\n",
    "journey to 2030 is focused on first reducing our scope 1, 2, and 3 greenhouse gas emissions by 50 percent compared with 2015, and investing in high-quality carbon removal solutions for the remaining emissions\"\n",
    "Output:\n",
    "<end_target>Emissions reduction target</end_target>\n",
    "<end_target_year>2030</end_target_year>\n",
    "<quote>reducing our greenhouse gas emissions by 50% compared to 2015 levels by the year 2030</quote>\n",
    "Input: \"Our long-term vision is to achieve net zero emissions across our entire value chain by 2050.\"\n",
    "Output:\n",
    "<end_target>Net zero</end_target>\n",
    "<end_target_year>2050</end_target_year>\n",
    "<quote>long-term vision is to achieve net zero emissions (...) by 2050.</quote>\n",
    "Input: \"We recognize the importance of environmental sustainability and are continuously working to improve our operations.\"\n",
    "Output:\n",
    "<end_target>No target</end_target>\n",
    "<end_target_year>No target</end_target_year>\n",
    "<quote>None</quote>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
