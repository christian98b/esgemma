We recommend to host the model in a production or local enviroment

For commercial environment choose a provider like VLLM. A good starting point is Runpod.io

For indivudial or test use choose a provider like Ollama or LM Studio