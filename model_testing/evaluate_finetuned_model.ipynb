{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook contains two main functions\n",
    "\n",
    "The first can be used when the eval function is run on the same device where the model got trained\n",
    "The second when the eval function is run on a device that needs to download the iterations from huggingface first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from core.run_evaluation import eval_function\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "##First load the dataset that is needed for testing \n",
    "test_dataset = load_dataset('chris7374/esg-net-zero', revision='test')\n",
    "df = test_dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just For Gemma2\n",
    "def generate_response(text : str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\" + text},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        tokenize=True\n",
    "    )\n",
    "    outputs = model.generate(inputs, max_new_tokens = 500, use_cache = True, pad_token_id=tokenizer.pad_token_id, temperature=0)\n",
    "    response_tokens = outputs[0][inputs.shape[-1]:]\n",
    "    response = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt\n",
    "instruction = \"\"\"You are an expert ESG (Environmental, Social, and Governance) analyst who conducts ESG research by analyzing texts to identify the presence of climate balance targets. Your primary task is to classify identified targets into one of four predefined classes and determine the target year for the climate balance target. Only consider overall climate balance targets, meaning that they are company-wide.\n",
    "The possible classes are “Carbon neutral(ity)”, “Emissions reduction target”, “Net zero”, and “No target”.\n",
    "Each class has equal importance, and the correct classification should reflect the most explicit target mentioned in the text. In cases where multiple classes are present:\n",
    "\t•\t“Net zero” should only be prioritized if explicitly mentioned as a company’s overarching target.\n",
    "\t•\t“Carbon neutral(ity)” takes precedence over “Emissions reduction target” only if it is the primary focus of the text.\n",
    "\t•\t“Emissions reduction target” should be classified if it is directly stated and not overshadowed by “Net zero” or “Carbon neutral(ity)” commitments.\n",
    "\t•\tIf no explicit target is mentioned, classify as “No target”.\n",
    "Ensure the classification is based on explicit information from the text, without assuming that one target implies another unless clearly stated.\"\"\"\n",
    "example = \"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##If Folders are already on the device\n",
    "max_seq_length = 8192 \n",
    "dtype = None # None for auto detection. \n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# Path to the local folder containing the model checkpoints\n",
    "local_model_folder = \"models\"\n",
    "\n",
    "# List of checkpoint folders to check\n",
    "checkpoints_to_check = [\n",
    "     'checkpoint-105',\n",
    "] ## Keep empty for checking every checkpoint\n",
    "\n",
    "# Get a list of all checkpoint folders in the local model folder\n",
    "checkpoint_folders = [f for f in os.listdir(local_model_folder) if os.path.isdir(os.path.join(local_model_folder, f))]\n",
    "\n",
    "for checkpoint_folder in checkpoint_folders:\n",
    "    if len(checkpoints_to_check) > 0 and checkpoint_folder not in checkpoints_to_check:\n",
    "        continue\n",
    "    try:\n",
    "        model_path = os.path.join(local_model_folder, checkpoint_folder)\n",
    "        print(f\"Checkpoint: {model_path}\")\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_path,\n",
    "            max_seq_length=max_seq_length,\n",
    "            dtype=dtype,\n",
    "            load_in_4bit=load_in_4bit,\n",
    "            device_map='auto'\n",
    "        )\n",
    "        FastLanguageModel.for_inference(model=model)\n",
    "        # Rest of your code remains the same\n",
    "        eval_function(instruction=instruction,example=example, iteration=checkpoint_folder, ground_truth_dataframe=df,target_column='end_target', target_year_column='end_target_year', context_column='custom_text', prompt_structures=['CIEKX'], generate_response=generate_response ,finetuned=True ,save_to_docx=True)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        # Delete model and tokenizer to free up memory\n",
    "        del model\n",
    "        del tokenizer\n",
    "\n",
    "        # Clear CUDA cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the model is online\n",
    "from huggingface_hub import snapshot_download\n",
    "from huggingface_hub import HfApi\n",
    "import shutil\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "path=\"model-to-test\"\n",
    "\n",
    "checkpoints_folder = \"checkpoint\"  # Replace with your folder path\n",
    "repo_id = \"chris7374/15-10-esgemma2b-3-epoch\"  # Your Hugging Face repository ID\n",
    "\n",
    "checkpoints_to_check=[\n",
    "    #'checkpoint-105',\n",
    "    #'checkpoint-52'\n",
    "] ## Keep empty for checking ever checkpoint\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Get all branches for the model\n",
    "branches = api.list_repo_refs(repo_id)\n",
    "branches = branches.branches\n",
    "for branch in branches:\n",
    "    if(len(checkpoints_to_check) > 0):\n",
    "        if(branch.ref.split('/')[-1] not in checkpoints_to_check):\n",
    "            continue\n",
    "    if branch.ref.split('/')[-1] != 'main':\n",
    "        try:    \n",
    "            print(f\"Branch: {branch.ref.split('/')[-1]}\")\n",
    "            snapshot_download(repo_id=repo_id, revision=branch.ref.split('/')[-1], local_dir='model-to-test')\n",
    "\n",
    "            model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "                path,\n",
    "                max_seq_length = max_seq_length,\n",
    "                dtype=dtype,\n",
    "                load_in_4bit=load_in_4bit,\n",
    "                device_map='auto'\n",
    "            )\n",
    "            FastLanguageModel.for_inference(model=model)\n",
    "            \n",
    "            eval_function(instruction=instruction,example=example, iteration=branch.ref.split('/')[-1], ground_truth_dataframe=df,target_column='end_target', target_year_column='end_target_year', context_column='custom_text', prompt_structures=['CIEKX'],generate_response=generate_response ,finetuned=True ,save_to_docx=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        finally:\n",
    "            # Delete model and tokenizer to free up memory\n",
    "            del model\n",
    "            del tokenizer\n",
    "\n",
    "            # Clear CUDA cache\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            shutil.rmtree('model-to-test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
